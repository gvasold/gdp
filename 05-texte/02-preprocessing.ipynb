{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59faa886",
   "metadata": {},
   "source": [
    "# Text Preprocessing für NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beece17",
   "metadata": {},
   "source": [
    "## NLP (Natural Language Processing)\n",
    "\n",
    "Natural language processing (NLP) ist ein Zweig der Informatik, oder genauer gesagt der künstlichen Intelligenz, der versucht, geschriebene und gesprochene Sprache für den Computer \"verstehbar\" zu machen.\n",
    "\n",
    "### Einige Aufgaben von NLP:\n",
    "\n",
    "* Automatische Übersetzung zwischen Sprachen\n",
    "* Spracherkennung \n",
    "* Autmatisches Beantworten von Fragen\n",
    "* Zusammenfassung von Texten\n",
    "* Sentiment Analysis: Stimmungsanalyse\n",
    "* Topic Modelling: thematische Exploration von Texten\n",
    "* Named Entity Recognition (NER): Erkennen von Personen, Orten, Firmen usw. in fortlaufenden Texten\n",
    "* Speech to text and text to speech: Gesprochende Texte in geschriebene Texte umwandeln und vice versa.\n",
    "* Automatische Erzeugung von Texten und Fragen (text and question generation)\n",
    "* Rechtschreibkorrektur/Fehlerkorrektur\n",
    "* Disambiguierung (word-sense disambiguation): \"Verstehen\" der korrekten Bedeutung von mehrdeutigen Wörtern und Ausdrücken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83af79",
   "metadata": {},
   "source": [
    "## Was ist Preprocessing von Texten?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ece85-6148-4468-b3b0-af28ca416cd4",
   "metadata": {},
   "source": [
    "Natürlichsprachige Texte befinden sich meist nicht in einem Zustand, der für die computergestützte Verarbeitung ideal ist. Daher müssen wir die Daten zuerst in einen Zustand bringen, der die eigentliche Analyse erleichtert, verbessert oder überhaupt erst möglich macht. Erfahrungsgemäß tendieren wir dazu, diesem Schritt nicht ausreichend Aufmerksamkeit zu widmen, was sich dann später im Analyseprozess rächt. Deshalb sehen wir uns hier typische Aufgaben des Preprocessings an.\n",
    "\n",
    "Für Computer ist das Verstehen natürlicher Sprache eine schwierige Aufgabe, weil die Daten nicht in strukturierter Form (wie etwa in einer Tabelle oder relationalen Datenbank) vorliegen. Diese Strings (z. B. Tweets, Artikel, Rezensionen, Romane, Gedichte, Theaterstücke) oder Sprachaufnahmen sind also in der Regel unstrukturiert. Durch die Vorverarbeitung wird der Text in eine für den Computer besser verdauliche Form gebracht, so dass unsere NLP-Methoden (z. B. auf der Grundlage von Statistik oder von Algorithmen des maschinellen Lernens) besser funktionieren.\n",
    "\n",
    "Eine erste Hürde stellt bereits das Problem dar, herauszufinden, in welcher Hinsicht unsere Daten \"unordentlich\" sind, damit wir diese sinnvoll bereinigen können. Dieser Schritt ist mühsam und oft langweilig. Dennoch dürfen wir nicht darauf verzichten: Wenn unsere Daten nicht sauber sind, werden auch die Ergebnisse unserer Analyse nicht sauber sein.\n",
    "\n",
    "Die Auswahl, Umsetzung und Kombination von Vorverarbeitungsschritten hängt stark von der Domäne und der zu untersuchenden Fragestellung ab. Daher müssen wir nicht alle Schritte auf jedes Problem anwenden. Oft braucht es mehrere Iterationszyklen (Preprocessing -> Processing -> Preprocessing -> Processing etc.), wenn wir bei der Analyse fehlende Bereinigungsschritte erkennen.\n",
    "\n",
    "In diesem Notebook werden einige der üblichen Vorverarbeitungsschritte behandelt. Abhängig von Ihren Daten und Ihrer Aufgabe (sowie der NLP-Methode, die Sie verwenden möchten) werden Sie nur einige oder vielleicht sogar alle Schritte in diesem Leitfaden benötigen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0311a20",
   "metadata": {},
   "source": [
    "### Häufige Preprocessing Schritte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc484cf7",
   "metadata": {},
   "source": [
    "* Umwandlung in Kleinbuchstaben (vereinheitlichte Schreibweise)\n",
    "* Tokenization (Texte in kleinere \"Tokens\" zerlegen)\n",
    "* Satzzeichen entfernen\n",
    "* URLs, Tags oder andere Steuerzeichen entfernen\n",
    "* Stopwords entfernen\n",
    "* Stemming (Wortstamm extrahieren)\n",
    "* Lemmatisierung (Flexionsformen auf Grundform bringen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed761135",
   "metadata": {},
   "source": [
    "## Preprocessing in der Praxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5592c",
   "metadata": {},
   "source": [
    "Für viele der folgenden Schritte gibt es fertige Funktionen in diversen Bibliotheken. Um ein tieferes Verständnis davon zu erzielen, was wir da eigentlich tun, demonstriere ich zunächst einmal ein paar typische Preprocessing-Schritte mit Mitteln der Standard Library. \n",
    "\n",
    "Natürlich sollten wie das Rad nicht immer neu erfinden, sondern in der praktischen Arbeit Bibliotheken einsetzen, wie beispielhaft im zweiten Teil beschrieben. Der Vorteil ist nicht nur die Zeitersparnis, sondern auch, dass die von Biblotheken bereit gestellten Funktionen in der Regel leistungsfähiger, optimierter und besser getestet sind.\n",
    "\n",
    "Zunächst importieren wir zwei Bibliotheken aus der Standard Library, die den Umgang mit Texten erleichtern:\n",
    "\n",
    "* string (https://docs.python.org/library/string.html) bietet für unsere Zwecke vor allem eine Reihe von nützlichen Konstanten wie etwa ``ascii_letters``, ``ascii_lowercase`` oder ``punctuation``.\n",
    "* re ist die Regex-Bibliothek aus der Standard Library (zu Regulären Ausdrücken und dem re-Modul gibt es ein eigenes Notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0032c93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518435b",
   "metadata": {},
   "source": [
    "### Text einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c7edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cactus.txt') as fh:\n",
    "    text = fh.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b96505",
   "metadata": {},
   "source": [
    "### Zeilenumbrüche entfernen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dbe01a",
   "metadata": {},
   "source": [
    "Zum Entfernen der Zeilenumbrüche gibt es mehrere Möglichkeiten, wir verwenden hier die replace Methode des String Objekts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223994b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\", \" \")\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f8120-8586-4e42-9539-ffe7376b6185",
   "metadata": {},
   "source": [
    "Eine andere, vermutlich bessere Möglichkeit (weil diese gleich auch mehrfach vorkommenden Whitespace auf ein Leerzeichen reduziert) wäre die ``sub`` Methode von re:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996808bf-8e01-4ddc-b194-dafdfd8c0976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the text from file again, because we aleady\n",
    "# have modified text in the cell above\n",
    "with open('data/cactus.txt') as fh:\n",
    "    text = fh.read()\n",
    "\n",
    "# replace any number of subsequent whitespace to a single space\n",
    "text = re.sub(r'\\s+', ' ', text)\n",
    "text[:140]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5490f6",
   "metadata": {},
   "source": [
    "### Alles in Groß- oder Kleinbuchstaben umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfdbbc6",
   "metadata": {},
   "source": [
    "Im Normalfall wollen wir diese Umwandlung, weil die Wörter dieselben sind, auch wenn sie am Satzanfang groß geschrieben werden. Wenn wir die Groß/Kleinschreibung vereinheitlichen, müssen wir später bei der Analyse keine eigenen Regeln dafür bilden (das Vokabular unserer Textdaten wird reduziert). Bei manchen Sprachen wie Deutsch ist es allerdings so, dass die Beibehaltung der Groß- und Kleinschreibung die Ergebnisse des Part-of-Speech-Taggings verbessern kann. Im Zweifelsfall lohnt es sich, die Ergebnisse zu vergleichen.\n",
    "\n",
    "Die dazu benötigten Methoden ``lower()`` und  ``upper()`` stehen als String-Methoden zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text.lower()\n",
    "lower_text[:140]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a995f2db",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c44f0a7",
   "metadata": {},
   "source": [
    "Als Tokenization bezeichnet man den Schritt, in dem ein Text in kleinere Einheiten wie z.B. Wörter oder Sätze zerlegt wird.\n",
    "\n",
    "Dazu können wir die ``split()`` Methode des String-Objekts verwenden. Diese Methode trennt einen String an jedem Vorkommen eines Substrings auf und liefert die einzelnen Teile als Liste. Im einfachsten Falle rufen wìr ``split()`` ohne Argument auf. Dabei verwendet ``split()`` Whitespace-Zeichen (d.h. Leerzeichen, Tabulatoren, Zeilenumbrüche usw.) als Trenner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ffd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = lower_text.split() \n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58539dcd-bc1c-439f-b69d-cf0a1754227a",
   "metadata": {},
   "source": [
    "Wollen wir statt dessen in Sätze zerlegen wollen, können wir bei ``split()`` den Punkt als Trennzeichen angeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9100a808-ccac-4250-9e1c-721223d7eb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = lower_text.split('.')\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ac22dd",
   "metadata": {},
   "source": [
    "Wir sehen hier allerdings, dass das Ergebnis teilweise unerwünschte Leezzeichen enthält. Die rühren daher, dass nach einem Punkt normalerweise ein Leerzeichen folgt. Das Problem könnten wir damit lösen,\n",
    "dass wir an ``'. '``  (Punkt Leerzeichen) splitten. Das birgt jedoch die Gefahr, dass das Leezeichen nach dem Punkt nicht immer vorhanden sein muss (etwa am Ende des Textes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6c0f1-44de-475e-a305-205b559e277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = lower_text.split('. ')\n",
    "sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c21dfa5-fca3-4ce1-b803-d3ca1892337f",
   "metadata": {},
   "source": [
    " Kehren wir also zur ersten Lösung zurück und entfernen die störenden Leezeichen mit der ``strip()`` Methode in einer List Comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0295cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = lower_text.split('.')\n",
    "stripped = [s.strip() for s in sentences]\n",
    "stripped[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ac87c",
   "metadata": {},
   "source": [
    "Bei genauerer Betrachtung sehen wir möglicherweise noch, dass es irgendwo leere Tokens gibt, hier am Ende der Liste. Da diese für unsere Analyse keinerlei Bedeutung haben, sollten wir sie ebenefalls entfernen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83488ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = [x for x in stripped if x != '']\n",
    "new_list[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a028a407",
   "metadata": {},
   "source": [
    "Eine Alternative besteht natürlich in der Anwendung Regulärer Ausdrücke. Hier splitten wir an Punkt, Fragezeichen und Rufzeichen, wobei beliebige viele Leerzeichen nach den Trennzeichen (die oben zum leeren Token geführt haben) ignoriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d741ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [sentence for sentence in re.split(r'[.?!]\\s*', lower_text) if sentence]\n",
    "sentences[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faf271a-c466-49cc-8d80-fd27a8278b5d",
   "metadata": {},
   "source": [
    "## Preprocessing mit NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27216da6-3c6c-44cf-9269-5bf33bf76e65",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "NLTK das *Natural Language Toolkit* (https://www.nltk.org/) ist eine populäre Python Bibliothek zur Verarbeitung natürlicher Sprache.\n",
    "Mit der folgenden Code-Zelle können Sie testen, ob ntlk auf Ihrem Computer bereits installiert ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55068bb9-5705-400c-ac5b-20f2b9a9172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab77e95-9536-493d-b994-df5561e5e59a",
   "metadata": {},
   "source": [
    "Falls Sie bei der Ausführung der Zelle eine Fehlermeldung erhalten haben, müssen Sie Sie es mit\n",
    "\n",
    "```\n",
    "pip install nltk\n",
    "```\n",
    "\n",
    "bzw.\n",
    "\n",
    "```\n",
    "conda install nltk \n",
    "```\n",
    "\n",
    "installieren. Grundsätzlich sollte das auch aus diesem Notebook funktionieren. Entfernen Sie dazu das Kommentarzeichen (#) in einer der beiden folgenden Zeilen und führen Sie dann die Zelle aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d9dbd-0657-4f13-8720-ba257f35c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#!conda install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb8f00f-d2ec-42a7-b016-d97e44fbf071",
   "metadata": {},
   "source": [
    "### Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ed6d2a-0e9e-42c0-bc96-43219eeeb5b7",
   "metadata": {},
   "source": [
    "Der oben beschriebene Ansatz, der sich auf Möglichkeiten der Standard Library beschränkt, empfiehlt sich nur für sehr einfache Anwendungsfälle. In der Praxis gibt es eine Reihe von Schwierigkeiten (z.B. Punkte als Dezimaltrenner, weitere Satzzeichen wie ! oder ?, dazu möglicherweise unerwartete Satzzeichen wie das spanische ``¿``, die die Aufgabe erschweren. Daher sollte man spezialisierte Bibliotheken verwenden. \n",
    "\n",
    "Das NLTK Package etwa bietet mehrere Tokenizer-Funktionen wie ``sent_tokenize()`` für Sätze oder ``word_tokenize()`` für Wörter, wo diese Spezialfälle zumindest zum Teil bereits mitgedacht sind. Damit das funktioniert, müssen wir (einmal) den``punkt`` Tokenizer des NLTK heruntladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c881ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')  # in case it has not been installed yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f119c1-0fb2-416d-b83d-f61a0326ce34",
   "metadata": {},
   "source": [
    "#### sent_tokenize\n",
    "\n",
    "Im nächsten Schritt verwenden wir die ``sent_tokenize()`` Funktion von nltk, um den Text in einzelne Sätze zu zerlegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9d9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentences = sent_tokenize(lower_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad9145a-9ee0-48d3-bbaf-7472e53c4880",
   "metadata": {},
   "source": [
    "Bei ``sent_tokenize()`` (wie bei allen `punkt` Tokenizern ist zu beachten, dass die Satzzeichen bewahrt werden. Falls wir an den Satzzeichen nicht interessiert sind, sollten wir diese in einem weiteren Schritt entfernen. Dabei machen wir uns zunutze, dass das string Modul der Standard Library die Satzzeichen als Konstante (``string.punctuation``) vordefiniert hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3cd435-53ee-4f72-a71c-620937ffce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_sentences = []\n",
    "for sentence in w_sentences:\n",
    "    if sentence[-1] in string.punctuation:\n",
    "        stripped_sentences.append(sentence[:-1].strip())\n",
    "    else:\n",
    "        stripped_sentences.append(sentence)\n",
    "stripped_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18329c-bc79-4da7-a98c-b34b6e553197",
   "metadata": {},
   "source": [
    "##### Sprachspezifische Tokenizer\n",
    "Die Tokenizer-Funktionen können auch an eine bestimmte Sprache (default ist Englisch) angepasst werden. Im folgenden Beispiel tokenisieren wir einen deutschsprachigen Roman von Jakob Wassermann: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4faf1e2-f8f6-4899-a61b-2602120a19bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wassermann/der_mann_von_vierzig_jahren.txt', encoding='utf-8') as fh:\n",
    "    w_text = re.sub(r'\\s+', ' ', fh.read().lower())\n",
    "w_sentences = sent_tokenize(w_text, language='german')\n",
    "w_sentences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508f7d42",
   "metadata": {},
   "source": [
    "#### word_tokenize\n",
    "\n",
    "Der Word-Tokenizer liefert eine Liste von Wörtern. Im folgenden Beispiel zerlegen wir den Text mit Hilfe der NLTK ``word_tokenize()``Funktion in einzelne Wörter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08522799",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(lower_text)\n",
    "words[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769ff32-3472-41e7-8ff9-cda961184b7d",
   "metadata": {},
   "source": [
    "Wir sehen, dass auch hier die Satzzeichen als Tokens erhalten bleiben.\n",
    "Je nach Fragestellung kann das nützlich sein oder auch nicht. Falls wir an den Satzzeichen nicht interessiert sind, können wir diese in einer List Comprehension entfernen. Auch hier machen wir uns zunutze, dass im ``string`` Modul die Satzzeichen als Konstante definiert sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cd6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token for token in words if token not in string.punctuation]\n",
    "words[:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25720746",
   "metadata": {},
   "source": [
    "### Stopwords entfernen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebcf81-2117-4ea7-a969-5059eeef842e",
   "metadata": {},
   "source": [
    "Stopwords sind eine Liste von Wörtern, an denen wir nicht interessiert sind. Das sind z.B. Füllwörter, Artikel und Pronomen. Wie können diese Listen selbst erstellen, was allerdings relativ mühsam ist. Besser sucht man sich eine im Internet bereit gestellte Stopwortliste für die entsprechende Sprache oder man verwendet z.B. eine der von NLTK bereit gestellte Liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741b1d61-091f-4f66-b4f1-471c599d7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')  # only needed once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a8f67-bfb8-4c99-a23b-6672b04051e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "german_stopwords = stopwords.words('german')\n",
    "print(f\"Die ersten 10 Stopwords: {german_stopwords[:10]}\")\n",
    "print(f\"Insgesamt sind {len(german_stopwords)} Stopwords definiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e81b9-b54b-419a-b1f7-36cabd7df6b1",
   "metadata": {},
   "source": [
    "In der Praxis nimmt man häufig eine solche bestehende Stopwortliste und passt sie an die eigenen Daten an, indem man weitere Stopwords hinzufügt. In unserem Beispiel verwenden wir eine solche individuelle Stopwortliste aus der Datei ``stopwords.txt`` im ``data`` Verzeichnis. Diese müssen wir natürlich einlesen und die einzelnen Stopwords als Liste bereitstellen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords.txt') as fp:\n",
    "    stopwords = [word.rstrip() for word in fp.readlines()]\n",
    "stopwords[:10]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e627b0ab",
   "metadata": {},
   "source": [
    "Dann verwenden wir wieder eine List Comprehension, um alle Stopwords aus unserer Tokens-Liste zu entfernen. Zur Kontrolle lassen wir uns die Zahl der Tokens vor und nach diesem Schritt ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a436ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words)) \n",
    "tokens = [token for token in words if token not in stopwords]\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa50e28-6f9b-4495-9b63-9fb9c097fffd",
   "metadata": {},
   "source": [
    "Wir haben also 70 Stopwords entfernt, die unsere anstehende Auswertung verwässert hätten. Das sind immerhin fast 50% der Tokens!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4610b64",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa326b58",
   "metadata": {},
   "source": [
    "Stemming nennt sich der Prozess, Wortformen auf ihren Wortstamm zu reduzieren. Dabei werden Präfixe und Suffixe entfernt. Da diese regelbasiert funktioniert, ist das Ergebnis nicht immer korrekt. Es ist aber in der Regel besser, mit diesen unvollkommenen Bereinigungen zu arbeiten als mit unbereinigten Daten.\n",
    "\n",
    "Beispiele:\n",
    "\n",
    "```\n",
    "books      --->    book\n",
    "looked     --->    look\n",
    "denied     --->    deni\n",
    "flies      --->    fli\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2803ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in tokens]\n",
    "# print the original token and the stammatized version\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"{token} -> {stems[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147ad07",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd1a74d",
   "metadata": {},
   "source": [
    "Lemmatisierung vereinheitlicht ebenfalls Wortformen auf ihre Grundform. Lemmatisierung ist aber ungleich mächtiger (und komplizierter) als Stemming, weil es eine morphologische Analyse durchführt. Es liefert das Lemma, also die Grundform aller Flexionsformen eines Wortes und berücksichtigt dabei Wissen über eine Sprache. Das geht so weit, dass sogar unregelmässige Formen wie das lateinische \n",
    "*tollere, sustulī, sublatum* korrekt behandelt werden. Lemmatisierung liefert also immer eine gültigen Wortform.\n",
    "\n",
    "Beispiele:\n",
    "\n",
    "```\n",
    "books      --->    book\n",
    "looked     --->    look\n",
    "denied     --->    deny\n",
    "flies      --->    fly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db57d88",
   "metadata": {},
   "source": [
    "NLTK stellt den WordNetLemmatizer bereit, über den wir Lemmatisierungen vornehmen können:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39c6e7-3759-415a-a52d-54977cea4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install data if not installed yet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')  # install data for POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f62c2dd-b54c-4b00-b015-4a6fc67191a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41403ffa",
   "metadata": {},
   "source": [
    "Damit die Lemmatisierung funktioniert, benötigen wir auch noch Part-Of-Speech Tagging (POS). Die ist nötig, weil die Lemmatisierung den grammatischen Kontext des Wortes benötigt. \n",
    "\n",
    "Ein POS-Tagger ergänzt das Token mit einem so genannten POS Tag, der die Funktion des Wortes festlegt ('ADJ', 'NOUN', usw.). Wir haben also nach dem POS-Tagging ein Tupel, das beispielsweise so aussieht: ``('water', 'NN')``. Leider verwenden unterschiedliche Tagger unterschiedliche und auch unterschiedlich differenzierte Tagsets. Die Bezeichnungen der Tags können sich also von Tagger zu Tagger unterscheiden. Der NLTK-Tagger liefert z.B. den String 'NN' für ein Nomen. \n",
    "\n",
    "Der WordNetLemmatizer erwartet hier aber einen Wert der sich (intern) hinter der Konstante ``wordnet.NOUN`` verbirgt. Deshalb müssen wir die Tag-Werte übersetzen, ehe wir sie an den Lemmatizer übergeben. Dazu verwenden wir die einfache Funktion\n",
    "`get_wordnet_pos()`. Diese ersetzt z.B. alle Tags, die mit `N` beginnen (daher auch ``NN``) durch den Wert von wordnet.NOUN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to map NTLK position tags \n",
    "# to wordnet tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bb3659-0d9e-4512-8498-60aaed9fbb9f",
   "metadata": {},
   "source": [
    "Führen wir zunächst das Part-of-Speech (POS) Tagging durch und lassen und das Ergebnis ausgeben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fe2cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tags\n",
    "word_pos_tags = nltk.pos_tag(tokens)\n",
    "word_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd474b57-68b1-4d9c-8234-617150085a13",
   "metadata": {},
   "source": [
    "Die Liste ``word_pos_tags`` enthält also zweiwertige Tupel: An Position 0 das Token und an Position 1 den ermittelten Position Tag (also die grammatische Funktion des Tokens im Satz:\n",
    "Verb, Noun, Adjective, Adverbe usw.)\n",
    "\n",
    "Im nächsten Schritt führen wir für jedes Token zwei Schritte durch:\n",
    "\n",
    "  1) Wir übersetzen den vom POS-Tagger gelieferten Position Tag für das \n",
    "     jeweilige Token in die Form, die der Wordnet Lemmatizer erwartet: \n",
    "     ``get_wordnet_pos(tag[1]``). Aus ``NNS`` wird dabei beispielsweise der \n",
    "     Wert der Konstante ``wordnet.NOUN``.\n",
    "  2) Wir übergeben das Token zusammen mit dem in Schritt 1 ermittelten Position \n",
    "     Tag an die ``lemmatize()`` Methode des Lemmatizers, die uns die lemmatisierte \n",
    "     Form des Tokens liefert. Das Ergebnis, d.h. die lemmatisierten Tokens speichern wir in die Liste\n",
    "     ``lemmatized_words``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d581424",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = []\n",
    "# Map the position tag and lemmatize the word/token\n",
    "for i, tag in enumerate(word_pos_tags):\n",
    "    lemmatized_words.append(lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3e5f0-1cd5-497e-836b-e916567cf45a",
   "metadata": {},
   "source": [
    "Zur Kontrolle können wir uns die Token und deren lemmatisierte Form zusammen ausgeben lassen. Das Ergebnis wirkt auf den ersten Blick etwas enttäuschend, weil die meisten Tokens im Text bereits in lemmatisierter Form erscheinen. Betrachten Sie aber beispielsweise ``looks``, ``growing``, ``tried`` oder ``asked``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49caae79-e9f2-4153-bf3d-0697ce078e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, token in enumerate(word_pos_tags):\n",
    "    print(f\"{token[0]} -> {lemmatized_words[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eef94ba-f089-4a7d-97b1-229bed0b9f4a",
   "metadata": {},
   "source": [
    "Wir können und die bereinigten und lemmatisierten Tokens auch wieder als Text ausgeben lassen. Dies ist der Text, den wir sinnvoll für diverse Analysen verwenden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf3e4e7-9b0d-4641-a2d4-8c8c43a46802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the lemmatize() method takes in the \n",
    "lemmatized_text = \" \".join(lemmatized_words)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaa2452",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Übung zum Preprocessing</b>\n",
    "<p>Preprozessieren Sie den Inhalt der Datei 'data/cat.txt'. Das Ergebnis sollte ein sauberer lemmatisierter Text sein.</p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
